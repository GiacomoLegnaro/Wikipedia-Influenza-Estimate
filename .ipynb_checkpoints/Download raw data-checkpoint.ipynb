{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "import shutil\n",
    "import requests\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "from random import choice\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing.dummy import Pool # use threads for I/O bound tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDirectory(path):\n",
    "    \"\"\"\n",
    "    Return the directory path where save the documents\n",
    "\n",
    "    :param path: path where create the new folder\n",
    "    :param directory: name of the new folder\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.join(path))\n",
    "    except OSError:\n",
    "        if not os.path.isdir(os.path.join(path)):\n",
    "            raise\n",
    "    return os.path.join(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_data(yr,mth,l_dl):\n",
    "    path = yr + '/' + mth + '/' + l_dl\n",
    "    urllib.URLopener.version = user_agents[link_dl.index(l_dl)%len(user_agents)]\n",
    "    wikimedia = urllib.URLopener() \n",
    "    headers = {\"Content-Length\": '213'}\n",
    "    while int(headers[\"Content-Length\"]) < 5000: \n",
    "        try:\n",
    "            filename, headers = wikimedia.retrieve(main_url + path, '/Users/giacomolegnaro/Downloads/Wikimedia/' + l_dl)\n",
    "        except IOError:\n",
    "            time.sleep(2)\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Progress ' + str(round(float(len(os.listdir('/Users/giacomolegnaro/Downloads/Wikimedia/'))) / len(link_dl)*100, 2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opener = urllib2.build_opener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_agents = [\n",
    "    'AmigaVoyager/3.2 (AmigaOS/MC680x0)',\n",
    "    'Cyberdog/2.0 (Macintosh; PPC)',\n",
    "    'IBM WebExplorer /v0.94',\n",
    "    'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',\n",
    "    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 7.0; America Online Browser 1.1; Windows NT 5.1; (R1 1.5); .NET CLR 2.0.50727; InfoPath.1)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 1.1.4322; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; Browzar)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/4.0; Deepnet Explorer 1.5.3; Smart 2x2; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; Avant Browser; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0)',\n",
    "    'Mozilla/4.08 (Charon; Inferno)',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "    'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.3) Gecko/20100402 Prism/1.0b4',\n",
    "    'Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; XH; rv:8.578.498) fr, Gecko/20121021 Camino/8.723+ (Firefox compatible)',\n",
    "    'Mozilla/5.0 (Macintosh; U; PPC Mac OS X; en) AppleWebKit/418.8 (KHTML, like Gecko, Safari) Cheshire/1.0.UNOFFICIAL',\n",
    "    'Mozilla/5.0 (Macintosh; U; PPC Mac OS X; pl-PL; rv:1.0.1) Gecko/20021111 Chimera/0.6',\n",
    "    'Mozilla/5.0 (Macintosh; U; PPC Mac OS X; pl-pl) AppleWebKit/312.8 (KHTML, like Gecko, Safari) DeskBrowse/1.0',\n",
    "    'Mozilla/5.0 (Macintosh; U; PPC; en-US; mimic; rv:9.3.0) Gecko/20120117 Firefox/3.6.25 Classilla/CFM',\n",
    "    'Mozilla/5.0 (Windows NT 5.2; RW; rv:7.0a1) Gecko/20091211 SeaMonkey/9.23a1pre',\n",
    "    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.24 (KHTML, like Gecko) RockMelt/0.9.58.494 Chrome/11.0.696.71 Safari/534.24',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.7 (KHTML, like Gecko) Comodo_Dragon/16.1.1.0 Chrome/16.0.912.63 Safari/535.7',\n",
    "    'Mozilla/5.0 (Windows; U; Win 9x 4.90; SG; rv:1.9.2.4) Gecko/20101104 Netscape/9.1.0285',\n",
    "    'Mozilla/5.0 (Windows; U; WinNT; en; rv:1.0.2) Gecko/20030311 Beonex/0.8.2-stable',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/532.2 (KHTML, like Gecko) ChromePlus/4.0.222.3 Chrome/4.0.222.3 Safari/532.2',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.2.3) Gecko/20100409 Firefox/3.6.3 CometBird/3.6.3',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; x64; fr; rv:1.9.2.13) Gecko/20101203 Firebird/3.6.13',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64; rv:10.0.11) Gecko/20100101 conkeror/1.0pre (Debian-1.0~~pre+git120527-1)',\n",
    "    'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.3a) Gecko/20021207 Phoenix/0.5',\n",
    "    'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "    'Mozilla/5.0 (X11; U; Linux i686; nl; rv:1.8.1b2) Gecko/20060821 BonEcho/2.0b2 (Debian-1.99+2.0b2+dfsg-1)',\n",
    "    'Mozilla/5.0 (X11; U; Linux x86_64; cs-CZ) AppleWebKit/533.3 (KHTML, like Gecko) rekonq Safari/533.3',\n",
    "    'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; SV1; Crazy Browser 9.0.04)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; AOL 9.7; AOLBuild 4343.19; Windows NT 6.1; WOW64; Trident/5.0; FunWebProducts)',\n",
    "    'Mozilla/5.0 (compatible; U; ABrowse 0.6; Syllable) AppleWebKit/420+ (KHTML, like Gecko)Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser 1.98.744; .NET CLR 3.5.30729)',\n",
    "    'Mozilla/6.0 (X11; U; Linux x86_64; en-US; rv:2.9.0.3) Gecko/2009022510 FreeBSD/ Sunrise/4.0.1/like Safari',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Sundance/0.9x(Compatible; Windows; U; en-US;)Version/0.9x'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2013']:\n",
    "    for m in ['-01']:\n",
    "        mth = yr + m\n",
    "\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        print(yr + ' ' + mth)\n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        makeDirectory('/Users/giacomolegnaro/Downloads/Wikimedia/')\n",
    "        makeDirectory('/Volumes/My Book/Wikimedia/' + sel + '/' + yr + '/' + mth + '/')\n",
    "\n",
    "        p = Pool(len(user_agents)*2)\n",
    "\n",
    "        out = p.map(lambda l_dl: download_data(yr,mth,l_dl), link_dl)\n",
    "\n",
    "        path = yr + '/' + mth + '/'\n",
    "        source = '/Users/giacomolegnaro/Downloads/Wikimedia/'\n",
    "        dest = '/Volumes/My Book/Wikimedia/' + sel + '/'\n",
    "        out = [shutil.move(source + f,dest + path + f) for f in os.listdir(source)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2012']:\n",
    "    for m in ['-12', '-11', '-10']:#, '-04', '-03', '-02', '-01']:\n",
    "        mth = yr + m\n",
    "\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        print(yr + ' ' + mth)\n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        makeDirectory('/Users/giacomolegnaro/Downloads/Wikimedia/')\n",
    "        makeDirectory('/Volumes/My Book/Wikimedia/' + sel + '/' + yr + '/' + mth + '/')\n",
    "\n",
    "        p = Pool(len(user_agents)*2)\n",
    "\n",
    "        out = p.map(lambda l_dl: download_data(yr,mth,l_dl), link_dl)\n",
    "\n",
    "        path = yr + '/' + mth + '/'\n",
    "        source = '/Users/giacomolegnaro/Downloads/Wikimedia/'\n",
    "        dest = '/Volumes/My Book/Wikimedia/' + sel + '/'\n",
    "        out = [shutil.move(source + f,dest + path + f) for f in os.listdir(source)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2016']:\n",
    "    for m in ['-10']:#, '-04', '-03', '-02', '-01']:\n",
    "        mth = yr + m\n",
    "\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        print(yr + ' ' + mth)\n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        makeDirectory('/Users/giacomolegnaro/Downloads/Wikimedia/')\n",
    "        makeDirectory('/Volumes/My Book/Wikimedia/' + sel + '/' + yr + '/' + mth + '/')\n",
    "\n",
    "        p = Pool(len(user_agents)*2)\n",
    "\n",
    "        out = p.map(lambda l_dl: download_data(yr,mth,l_dl), link_dl)\n",
    "\n",
    "        path = yr + '/' + mth + '/'\n",
    "        source = '/Users/giacomolegnaro/Downloads/Wikimedia/'\n",
    "        dest = '/Volumes/My Book/Wikimedia/' + sel + '/'\n",
    "        out = [shutil.move(source + f,dest + path + f) for f in os.listdir(source)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2012']:\n",
    "    for m in ['-04', '-03', '-02', '-01']:\n",
    "        mth = yr + m\n",
    "\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        print(yr + ' ' + mth)\n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        makeDirectory('/Users/giacomolegnaro/Downloads/Wikimedia/')\n",
    "        makeDirectory('/Volumes/My Passport/Wikimedia/' + sel + '/' + yr + '/' + mth + '/')\n",
    "\n",
    "        p = Pool(len(user_agents)*2)\n",
    "\n",
    "        out = p.map(lambda l_dl: download_data(yr,mth,l_dl), link_dl)\n",
    "\n",
    "        path = yr + '/' + mth + '/'\n",
    "        source = '/Users/giacomolegnaro/Downloads/Wikimedia/'\n",
    "        dest = '/Volumes/My Passport/Wikimedia/' + sel + '/'\n",
    "        out = [shutil.move(source + f,dest + path + f) for f in os.listdir(source)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2011']:\n",
    "    for m in ['-11', '-10']:\n",
    "        mth = yr + m\n",
    "\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        print(yr + ' ' + mth)\n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        makeDirectory('/Users/giacomolegnaro/Downloads/Wikimedia/')\n",
    "        makeDirectory('/Volumes/My Passport/Wikimedia/' + sel + '/' + yr + '/' + mth + '/')\n",
    "\n",
    "        p = Pool(len(user_agents)*2)\n",
    "\n",
    "        out = p.map(lambda l_dl: download_data(yr,mth,l_dl), link_dl)\n",
    "\n",
    "        path = yr + '/' + mth + '/'\n",
    "        source = '/Users/giacomolegnaro/Downloads/Wikimedia/'\n",
    "        dest = '/Volumes/My Passport/Wikimedia/' + sel + '/'\n",
    "        out = [shutil.move(source + f,dest + path + f) for f in os.listdir(source)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'Amantadina',\n",
    "    'Antivirale',\n",
    "    'Artralgia',\n",
    "    'Cefalea',\n",
    "    'Febbre',\n",
    "    'Fotofobia',\n",
    "    'Influenza',\n",
    "    'Influenzavirus_A',\n",
    "    'Influenzavirus_A_sottotipo_H1N1',\n",
    "    'Mialgia',\n",
    "    'Oseltamivir',\n",
    "    'Pandemia_influenzale',\n",
    "    'Paracetamolo',\n",
    "    'Rinorrea',\n",
    "    'Starnuto',\n",
    "    'Tosse',\n",
    "    'Vaccino_antinfluenzale',\n",
    "    \"Virus_dell'influenza_A_sottotipo_H1N1\",\n",
    "    'Vomito',\n",
    "    'Zanamivir'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cStringIO\n",
    "import gzip\n",
    "import subprocess\n",
    "import time\n",
    "from itertools import islice\n",
    "from StringIO import StringIO\n",
    "\n",
    "io_method = cStringIO.StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE_DIR = '/Users/giacomolegnaro/Documents/Data Science/2nd Year/Digital Epidemiology/Homework2/Wikimedia/stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_data(yr,mth,l_dl):\n",
    "    path = yr + '/' + mth + '/' + l_dl\n",
    "    urllib.URLopener.version = user_agents[link_dl.index(l_dl)%len(user_agents)]\n",
    "    wikimedia = urllib.URLopener() \n",
    "    headers = {\"Content-Length\": '213'}\n",
    "    while int(headers[\"Content-Length\"]) < 5000: \n",
    "        try:\n",
    "            filename, headers = wikimedia.retrieve(main_url + path, '/Users/giacomolegnaro/Downloads/Wikimedia/' + l_dl)\n",
    "        except IOError:\n",
    "            time.sleep(2)\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Progress ' + str(round(float(len(os.listdir('/Users/giacomolegnaro/Downloads/Wikimedia/'))) / len(link_dl)*100, 2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadAnalyzeData(yr,mth,l_dl):\n",
    "    path = yr + '/' + mth + '/' + l_dl\n",
    "    flag = True\n",
    "    while flag:\n",
    "        try:\n",
    "            #opener = urllib2.build_opener()\n",
    "            #opener.addheaders = [('User-Agent', user_agents[link_dl.index(l_dl)%len(user_agents)])]\n",
    "            request = urllib2.Request(main_url + path)\n",
    "            request.add_header('Accept-encoding', 'gzip')\n",
    "            request.add_header = ('User-Agent', user_agents[link_dl.index(l_dl)%len(user_agents)])\n",
    "            response = urllib2.urlopen(request)\n",
    "            buf = StringIO(response.read())\n",
    "            f = gzip.GzipFile(fileobj=buf)\n",
    "            flag = False\n",
    "        except IOError:\n",
    "                time.sleep(2)\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Download done!')\n",
    "    ct = 0\n",
    "    fileName = (main_url + path).split('/')[-1][:-3].split('-')\n",
    "    for line in f.readlines():\n",
    "        lineList = line.split(' ')\n",
    "        if lineList[0] == 'it':\n",
    "            try:\n",
    "                ct += int(lineList[2])\n",
    "            except IndexError:\n",
    "                ct += 0\n",
    "            if urllib.unquote(lineList[1]) in keywords:\n",
    "                with open(FILE_DIR + '/' + urllib.unquote(lineList[1]) + '.txt', 'a') as output:\n",
    "                    output.write(fileName[1] + fileName[2] + ' ' + lineList[2] + '\\n')\n",
    "        elif ((lineList[0] != 'it') and (ct != 0)):\n",
    "            break\n",
    "\n",
    "    with open (FILE_DIR + '/' 'totalViews.txt','a') as output:\n",
    "        output.write(fileName[1] + fileName[2] + ' ' + str(ct) + '\\n')\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Analysis done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-04\n",
      "2011 2011-04\n"
     ]
    }
   ],
   "source": [
    "yr = '2011'\n",
    "m = '-04'\n",
    "mth = yr + m\n",
    "print(mth)\n",
    "if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "    sel = 'pageviews'\n",
    "else:\n",
    "    sel = 'pagecounts-raw'\n",
    "main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "print(yr + ' ' + mth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "print len(link_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 2011-04 pagecounts-20110401-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-120000.gz - Analysis done!\n"
     ]
    }
   ],
   "source": [
    "d = downloadAnalyzeData(yr,mth,link_dl[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 2011-04\n",
      "720\n",
      "2011 2011-04 pagecounts-20110401-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110402-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110402-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110406-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110402-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110406-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110402-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-120001.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110402-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110402-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110406-150000.gz - Download done!2011 2011-04 pagecounts-20110406-120000.gz - Download done!\n",
      "\n",
      "2011 2011-04 pagecounts-20110401-030000.gz - Analysis done!\n",
      "2011 2011-04 pagecounts-20110402-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110406-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-150000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110408-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110403-090000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110406-060000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110406-180000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110410-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-210000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110407-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110409-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110406-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110401-040000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-120000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110402-030000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110404-000000.gz - Download done!\n",
      "2011 2011-04 pagecounts-20110405-030000.gz - Download done!\n"
     ]
    }
   ],
   "source": [
    "for yr in ['2011']:\n",
    "    for m in ['-04']:\n",
    "        mth = yr + m\n",
    "\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        print(yr + ' ' + mth)\n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "\n",
    "        p = Pool(len(user_agents)*2)\n",
    "        out = p.map(lambda l_dl: downloadAnalyzeData(yr,mth,l_dl), link_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = yr + '/' + mth + '/' + link_dl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opener = urllib2.build_opener()\n",
    "opener.addheaders = [('User-Agent', user_agents[link_dl.index(link_dl[0])%len(user_agents)])]\n",
    "request = urllib2.Request(main_url + path)\n",
    "request.add_header('Accept-encoding', 'gzip')\n",
    "response = urllib2.urlopen(request)\n",
    "buf = StringIO(response.read())\n",
    "f = gzip.GzipFile(fileobj=buf)\n",
    "data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for d in islice(data, 3500000, None):\n",
    "    ct += 1\n",
    "    print d\n",
    "    if ct > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urllib2.URLopener.version = user_agents[link_dl.index(link_dl[0])%len(user_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "request = urllib2.Request(main_url + path)\n",
    "request.add_header('Accept-encoding', 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = yr + '/' + mth + '/' + l_dl\n",
    "urllib2.URLopener.version = user_agents[link_dl.index(l_dl)%len(user_agents)]\n",
    "request = urllib2.Request(main_url + path)\n",
    "request.add_header('Accept-encoding', 'gzip')\n",
    "response = urllib2.urlopen(request)\n",
    "buf = StringIO(response.read())\n",
    "f = gzip.GzipFile(fileobj=buf)\n",
    "data = f.readlines()\n",
    "print(yr + ' ' + mth + ' ' + l_dl + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request = urllib2.Request('https://dumps.wikimedia.org/other/pageviews/2016/2016-05/pageviews-20160501-050000.gz')\n",
    "request.add_header('Accept-encoding', 'gzip')\n",
    "response = urllib2.urlopen(request)\n",
    "buf = StringIO(response.read())\n",
    "f = gzip.GzipFile(fileobj=buf)\n",
    "data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_data(yr,mth,link_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  To download not ok files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yr = '2011'\n",
    "m = '-12'\n",
    "mth = yr + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "    sel = 'pageviews'\n",
    "else:\n",
    "    sel = 'pagecounts-raw'\n",
    "main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(yr + ' ' + mth)\n",
    "r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "file_dl = os.listdir('/Users/giacomolegnaro/Downloads/Wikimedia/')\n",
    "to_dl = list(set(link_dl) - set(file_dl))\n",
    "to_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(yr,mth,l_dl):\n",
    "    path = yr + '/' + mth + '/' + l_dl\n",
    "    urllib.URLopener.version = user_agents[link_dl.index(l_dl)%len(user_agents)]\n",
    "    wikimedia = urllib.URLopener() \n",
    "    wikimedia.retrieve(main_url + path, '/Users/giacomolegnaro/Downloads/Wikimedia/' + l_dl)\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Progress ' + str(round(float(len(os.listdir('/Users/giacomolegnaro/Downloads/Wikimedia/'))) / len(link_dl)*100, 2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l_dl in to_dl:\n",
    "    download_data(yr,mth,l_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = yr + '/' + mth + '/'\n",
    "source = '/Users/giacomolegnaro/Downloads/Wikimedia/'\n",
    "dest = '/Volumes/My Passport/Wikimedia/' + sel + '/'\n",
    "out = [shutil.move(source + f,dest + path + f) for f in os.listdir(source)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from StringIO import StringIO\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "request = urllib2.Request('https://dumps.wikimedia.org/other/pageviews/2016/2016-05/pageviews-20160501-050000.gz')\n",
    "request.add_header('Accept-encoding', 'gzip')\n",
    "response = urllib2.urlopen(request)\n",
    "buf = StringIO(response.read())\n",
    "f = gzip.GzipFile(fileobj=buf)\n",
    "data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for d in islice(data, 3500000, None):\n",
    "    ct += 1\n",
    "    print d\n",
    "    if ct > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in islice(fh, 3500000, None):\n",
    "    lineList = line.split(' ')\n",
    "    if lineList[0] == 'it':\n",
    "        try:\n",
    "            ct += int(lineList[2])\n",
    "        except IndexError:\n",
    "            ct += 0\n",
    "        if lineList[1] in keywords:\n",
    "            with open(FILE_DIR + '/' + lineList[1] + '.txt', 'a') as output:\n",
    "                output.write(fileName[1] + fileName[2] + ' ' + lineList[2] + '\\n')\n",
    "    elif ((lineList[0] != 'it') and (ct != 0)):\n",
    "        break\n",
    "\n",
    "with open (FILE_DIR + '/' 'totalViews.txt','a') as output:\n",
    "    output.write(fileName[1] + fileName[2] + ' ' + str(ct) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codice Sporco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(main_url)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "year = [link.get('href').encode('utf8') for link in soup.find_all('a') if link.get('href') == link.string]\n",
    "#for yr in year:\n",
    "#yr = '2016'\n",
    "r = requests.get(main_url + yr + '/') \n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "month = [link.get('href').encode('utf8') for link in soup.find_all('a') if link.get('href') == link.string]\n",
    "#for mth in month:\n",
    "#mth = '2016-02'\n",
    "print(yr + ' ' + mth)\n",
    "r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "print len(link_dl)\n",
    "makeDirectory('/Users/giacomolegnaro/Downloads/Wikimedia/')\n",
    "makeDirectory('/Volumes/My Book/Wikimedia/pageviews/' + yr + '/' + mth + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def download_data(yr,mth,l_dl):\n",
    "    path = yr + '/' + mth + '/' + l_dl\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Progress ' + str(round(float(link_dl.index(l_dl)) / len(link_dl)*100, 2))+'%')\n",
    "    print 'add_header'\n",
    "    opener.addheaders = {('User-Agent': user_agents[link_dl.index(l_dl)%len(user_agents)])}\n",
    "    print main_url + path\n",
    "    response = opener.open(main_url + path, '/Users/giacomolegnaro/Downloads/Wikimedia/' + l_dl)\n",
    "    source = '/Users/giacomolegnaro/Downloads/Wikimedia/'+l_dl\n",
    "    dest = '/Volumes/My Book/Wikimedia/pageviews/'+ path\n",
    "    #shutil.move(source,dest) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
