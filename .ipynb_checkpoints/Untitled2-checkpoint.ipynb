{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import urllib2\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "from StringIO import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'Amantadina',\n",
    "    'Antivirale',\n",
    "    'Artralgia',\n",
    "    'Cefalea',\n",
    "    'Febbre',\n",
    "    'Fotofobia',\n",
    "    'Influenza',\n",
    "    'Influenzavirus_A',\n",
    "    'Influenzavirus_A_sottotipo_H1N1',\n",
    "    'Mialgia',\n",
    "    'Oseltamivir',\n",
    "    'Pandemia_influenzale',\n",
    "    'Paracetamolo',\n",
    "    'Rinorrea',\n",
    "    'Starnuto',\n",
    "    'Tosse',\n",
    "    'Vaccino_antinfluenzale',\n",
    "    \"Virus_dell'influenza_A_sottotipo_H1N1\",\n",
    "    'Zanamivir'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE_DIR = '/Users/giacomolegnaro/Documents/Data Science/2nd Year/Digital Epidemiology/Homework2/Wikimedia/stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadAnalyzeData(yr,mth,l_dl):\n",
    "    path = yr + '/' + mth + '/' + l_dl\n",
    "    flag = True\n",
    "    while flag:\n",
    "        try:\n",
    "            #opener = urllib2.build_opener()\n",
    "            #opener.addheaders = [('User-Agent', user_agents[link_dl.index(l_dl)%len(user_agents)])]\n",
    "            request = urllib2.Request(main_url + path)\n",
    "            request.add_header('Accept-encoding', 'gzip')\n",
    "            response = urllib2.urlopen(request)\n",
    "            buf = StringIO(response.read())\n",
    "            f = gzip.GzipFile(fileobj=buf)\n",
    "            flag = False\n",
    "        except IOError:\n",
    "                time.sleep(2)\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Download done!')\n",
    "    ct = 0\n",
    "    fileName = (main_url + path).split('/')[-1][:-3].split('-')\n",
    "    for line in f.readlines():\n",
    "        lineList = line.split(' ')\n",
    "        if lineList[0] == 'it':\n",
    "            try:\n",
    "                ct += int(lineList[2])\n",
    "            except IndexError:\n",
    "                ct += 0\n",
    "            if urllib.unquote(lineList[1]) in keywords:\n",
    "                with open(FILE_DIR + '/' + urllib.unquote(lineList[1]) + '.txt', 'a') as output:\n",
    "                    output.write(fileName[1] + fileName[2] + ' ' + lineList[2] + '\\n')\n",
    "        elif ((lineList[0] != 'it') and (ct != 0)):\n",
    "            break\n",
    "\n",
    "    with open (FILE_DIR + '/' 'totalViews.txt','a') as output:\n",
    "        output.write(fileName[1] + fileName[2] + ' ' + str(ct) + '\\n')\n",
    "    print(yr + ' ' + mth + ' ' + l_dl + ' - Analysis done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2016']:\n",
    "    for m in ['-04','-03','-02','-01']:\n",
    "        mth = yr + m\n",
    "        print(mth)\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "\n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        for l_dl in link_dl:\n",
    "            d = downloadAnalyzeData(yr,mth,l_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2015','2014','2013']:\n",
    "    for m in ['-12','-11','-10','-04','-03','-02','-01']:\n",
    "        mth = yr + m\n",
    "        print(mth)\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        \n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        for l_dl in link_dl:\n",
    "            d = downloadAnalyzeData(yr,mth,l_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in ['2012']:\n",
    "    for m in ['-12','-11','-10']:\n",
    "        mth = yr + m\n",
    "        print(mth)\n",
    "        if (yr == '2016') or ((yr == '2015') and (m not in ['-01', '-02', '-03', '-04'])):\n",
    "            sel = 'pageviews'\n",
    "        else:\n",
    "            sel = 'pagecounts-raw'\n",
    "        main_url = 'https://dumps.wikimedia.org/other/' + sel + '/'\n",
    "        \n",
    "        r = requests.get(main_url + yr + '/' + mth + '/')\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        link_dl = [link.get('href').encode('utf8') for link in soup.find_all('a') if ((link.get('href') == link.string) and (link.string.split('-')[0] == sel.split('-')[0]))]\n",
    "        print len(link_dl)\n",
    "        for l_dl in link_dl:\n",
    "            d = downloadAnalyzeData(yr,mth,l_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
